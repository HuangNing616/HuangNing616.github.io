<!DOCTYPE html>



<html lang="zh-CN">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Pisces',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="Spark是基于内存的计算框架，它主要包括SparkCore，SparkSQL，SparkML，SparkStreaming等方面的内容，本片文章从SparkSQL的角度出发，从SparkSQL的发展，DataSet，DataFrame，UDF以及UDAF这几个方面来介绍Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSQL学习笔记">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2020&#x2F;05&#x2F;23&#x2F;SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0&#x2F;index.html">
<meta property="og:site_name" content="黄宁の博客">
<meta property="og:description" content="Spark是基于内存的计算框架，它主要包括SparkCore，SparkSQL，SparkML，SparkStreaming等方面的内容，本片文章从SparkSQL的角度出发，从SparkSQL的发展，DataSet，DataFrame，UDF以及UDAF这几个方面来介绍Spark">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2020&#x2F;05&#x2F;23&#x2F;SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0&#x2F;谓词下推.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2020&#x2F;05&#x2F;23&#x2F;SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0&#x2F;嵌套格式.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2020&#x2F;05&#x2F;23&#x2F;SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0&#x2F;嵌套数据格式.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2020&#x2F;05&#x2F;23&#x2F;SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0&#x2F;json数组格式.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2020&#x2F;05&#x2F;23&#x2F;SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0&#x2F;json数组数据格式.png">
<meta property="article:published_time" content="2020-05-23T10:51:19.000Z">
<meta property="article:modified_time" content="2020-06-27T16:16:35.821Z">
<meta property="article:author" content="黄宁">
<meta property="article:tag" content="Spark学习记录｜计算机学习笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2020&#x2F;05&#x2F;23&#x2F;SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0&#x2F;谓词下推.png">

<link rel="canonical" href="http://yoursite.com/2020/05/23/SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>SparkSQL学习笔记 | 黄宁の博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
	<a href="https://github.com/HuangNing616" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">黄宁の博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">算法攻城狮</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
  </ul>

</nav>
</div>
    </header>



    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/23/SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E4%BA%91%E5%8D%97%E5%B1%B1%E9%97%B4%E8%87%AA%E6%8B%8D.jpg">
      <meta itemprop="name" content="黄宁">
      <meta itemprop="description" content="普通人也可以活得很精彩">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="黄宁の博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SparkSQL学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-05-23 18:51:19" itemprop="dateCreated datePublished" datetime="2020-05-23T18:51:19+08:00">2020-05-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-28 00:16:35" itemprop="dateModified" datetime="2020-06-28T00:16:35+08:00">2020-06-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="url" rel="index">
                    <span itemprop="name">计算机</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/" itemprop="url" rel="index">
                    <span itemprop="name">分布式计算</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Spark是基于<strong>内存</strong>的计算框架，它主要包括<strong>SparkCore，SparkSQL，SparkML，SparkStreaming</strong>等方面的内容，本片文章从SparkSQL的角度出发，从SparkSQL的发展，DataSet，DataFrame，UDF以及UDAF这几个方面来介绍Spark</p>
<a id="more"></a>
<h1 id="SparkSQL的发展"><a href="#SparkSQL的发展" class="headerlink" title="SparkSQL的发展"></a>SparkSQL的发展</h1><p>SparkSQL是支持<strong>使用SQL来查询分布式数据</strong>的技术栈，它的发展过程是<strong>Hive  -&gt; Shark -&gt; SparkSQL</strong></p>
<h2 id="与Hive的区别"><a href="#与Hive的区别" class="headerlink" title="与Hive的区别"></a>与Hive的区别</h2><p>SparkSQL中写的sql底层解析成Sparkjob，Hive中写的hql底层解析成MRjob</p>
<h2 id="与Shark的区别"><a href="#与Shark的区别" class="headerlink" title="与Shark的区别"></a>与Shark的区别</h2><ul>
<li>Shark中仅支持Hive的语法，SparkSQL中兼容所有Hive和Shark的语法</li>
<li>SparkSQL的出现是Spark完全脱离Hive，解耦，不再依赖于Hive的解析优化</li>
<li>SparkSQL支持查询原生的RDD，还可以将结果拿回当作RDD使用</li>
<li><strong>Spark on Hive</strong> — SparkSQL 中 Spark负责解析优化（sql语句），执行引擎(描述解析成xxxjob)，Hive只负责存储，而 Hive on Spark — Shark 中 Spark只负责执行引擎，Hive负责解析优化以及存储</li>
</ul>
<h2 id="引入了谓词下推来优化job"><a href="#引入了谓词下推来优化job" class="headerlink" title="引入了谓词下推来优化job"></a>引入了<strong>谓词下推</strong>来优化job</h2><p>在进行下述查询时</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">	table1.name, </span><br><span class="line">	table2.score</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">	table1 <span class="keyword">JOIN</span> table2</span><br><span class="line"><span class="keyword">ON</span></span><br><span class="line">	table1.id = table2.id</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">	table1.age &gt; <span class="number">50</span> <span class="keyword">AND</span> table2.score &gt; <span class="number">90</span></span><br></pre></td></tr></table></figure>
<p>不使用谓词下推和使用谓词下推的执行过程如下</p>
<p><img src="/2020/05/23/SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/谓词下推.png" alt></p>
<h2 id="版本1-6和2-0-的区别"><a href="#版本1-6和2-0-的区别" class="headerlink" title="版本1.6和2.0+的区别"></a>版本1.6和2.0+的区别</h2><ol>
<li>创建sql对象的方式<ul>
<li>Spark 1.6 — 创建SQLContext</li>
<li>Spark2.0+ — SparkSession</li>
</ul>
</li>
<li>得到DataFrame后注册临时表<ul>
<li>spark1.6 —— df.registerTempTable</li>
<li>spark2.0+ —— df.createOrReplaceTempView/ df.createOrReplaceGlobalTempView</li>
</ul>
</li>
</ol>
<h1 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h1><h2 id="与RDD的区别"><a href="#与RDD的区别" class="headerlink" title="与RDD的区别"></a>与RDD的区别</h2><ol>
<li>DataSet 内部序列化机制与RDD不同，它没有使用 java 序列化或者 Kryo 序列化，而是使用专门的编码器（负责<strong>将对象转换为字节</strong>）序列化对象，以便对象通过网络进行处理或传输。并允许Spark执行许多操作(如过滤、排序和哈希)且<strong>不用反序列</strong>成对象来调用对象的方法，加快计算时间</li>
<li>DataSet是<strong>强类型</strong>的，默认列名是“value”，操作上的<strong>方法比RDD的多</strong>，RDD中有的算子在DataSet中都有</li>
</ol>
<h2 id="创建DataSet"><a href="#创建DataSet" class="headerlink" title="创建DataSet"></a>创建DataSet</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// spark1.6+ 引入DataSet</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">name:<span class="type">String</span>,age:<span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Person</span>(<span class="params">id:<span class="type">Int</span>,name:<span class="type">String</span>,age:<span class="type">Int</span>,score:<span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">创建spark对象</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">spark</span></span>: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .master(<span class="string">"local"</span>)</span><br><span class="line">  .appName(<span class="string">"createStruceDataSet"</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法1: 根据scala集合创建DataSet</span></span><br><span class="line"><span class="keyword">val</span> list: immutable.<span class="type">Seq</span>[<span class="type">Person</span>] = <span class="type">List</span>[<span class="type">Person</span>](</span><br><span class="line">  <span class="type">Person</span>(<span class="number">1</span>,<span class="string">"zhangsan"</span>,<span class="number">18</span>,<span class="number">100</span>),</span><br><span class="line">  <span class="type">Person</span>(<span class="number">2</span>,<span class="string">"lisi"</span>,<span class="number">19</span>,<span class="number">200</span>),</span><br><span class="line">  <span class="type">Person</span>(<span class="number">3</span>,<span class="string">"wangwu"</span>,<span class="number">20</span>,<span class="number">300</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> ds1: <span class="type">Dataset</span>[<span class="type">Person</span>] = list.toDS()  <span class="comment">// 将List映射成Person类型的DataSet</span></span><br><span class="line"><span class="keyword">val</span> ds2: <span class="type">Dataset</span>[<span class="type">Int</span>] = <span class="type">List</span>[<span class="type">Int</span>](<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>).toDS() <span class="comment">// 将List映射成Int类型的DataSet</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法2：由json文件和类直接映射成DataSet</span></span><br><span class="line"><span class="keyword">val</span> ds3: <span class="type">Dataset</span>[<span class="type">Student</span>] = spark.read.json(<span class="string">"./data/json"</span>).as[<span class="type">Student</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法3：读取外部文件直接加载DataSet</span></span><br><span class="line"><span class="keyword">val</span> ds4: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(<span class="string">"./data/people.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> ds5: <span class="type">Dataset</span>[<span class="type">Person</span>] = dataSet.map(line =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> arr: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">","</span>)</span><br><span class="line">  <span class="type">Person</span>(arr(<span class="number">0</span>).toInt, arr(<span class="number">1</span>).toString, arr(<span class="number">2</span>).toInt, arr(<span class="number">3</span>).toDouble)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="实现WordCount"><a href="#实现WordCount" class="headerlink" title="实现WordCount"></a>实现WordCount</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> linesDs: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(<span class="string">"./data/words"</span>).as[<span class="type">String</span>]</span><br><span class="line"><span class="keyword">val</span> words: <span class="type">Dataset</span>[<span class="type">String</span>] = linesDs.flatMap(line=&gt;&#123;line.split(<span class="string">" "</span>)&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 统计每个单词的个数并从大到小排序</span></span><br><span class="line"><span class="keyword">val</span> groupDs: <span class="type">RelationalGroupedDataset</span> = words.groupBy($<span class="string">"value"</span> as <span class="string">"word"</span>)</span><br><span class="line"><span class="keyword">val</span> aggDs: <span class="type">DataFrame</span> = groupDs.agg(count(<span class="string">"*"</span>) as <span class="string">"totalCount"</span>)</span><br><span class="line"><span class="keyword">val</span> result: <span class="type">Dataset</span>[<span class="type">Row</span>] = aggDs.sort($<span class="string">"totalCount"</span> desc)</span><br><span class="line">result.show(<span class="number">100</span>, truncate = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过sparkSQL语句统计每个单词的个数并从大到小排序</span></span><br><span class="line"><span class="keyword">val</span> frame: <span class="type">DataFrame</span> = words.withColumnRenamed(<span class="string">"value"</span>,<span class="string">"word"</span>)</span><br><span class="line">frame.createOrReplaceTempView(<span class="string">"myWords"</span>)</span><br><span class="line">spark.sql(<span class="string">"select word,count(word) as totalCount from myWords group by word order by totalCount desc"</span>).show()</span><br></pre></td></tr></table></figure>
<h1 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h1><p>SparkCore底层操作的是<strong>RDD</strong>，SparkSQL底层操作的就是<strong>DataFrame</strong>，DataFrame像是一张二维表，有数据，也有列的Schema信息。想要使用sql查询分布式数据，<strong>首先创建DataFrame，然后注册视图，最后使用sql查询</strong>。 想要创建DataFrame 在<strong>Spark1.6中需要创建SQLContext</strong>，在<strong>Spark2.0+ 需要创建SparkSession</strong></p>
<p><strong>补充：</strong></p>
<ol>
<li>spark1.6里面scala和java都叫DataFrame</li>
<li>spark2.0+ java中的DataFrame叫<strong>DataSet</strong></li>
</ol>
<h2 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h2><p>为了使用sparkSQL，必须要创建DataFrame并且创造相应的视图</p>
<h3 id="读取json格式的文件"><a href="#读取json格式的文件" class="headerlink" title="读取json格式的文件"></a>读取json格式的文件</h3><ol>
<li><p>读取json格式文件，json中的属性名自动成为列并且按照Ascii排序，列的类型会自动推断</p>
</li>
<li><p><strong>读取json格式文件</strong>的两种方式</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建SparkSession对象</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">.appName(<span class="string">"DataSetFromJsonFile"</span>)</span><br><span class="line">.master(<span class="string">"local"</span>)</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式1</span></span><br><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"./data/json"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式2</span></span><br><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"./data/json"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>嵌套格式的json数据</strong>，比如</p>
<p><img src="/2020/05/23/SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/嵌套格式.png" alt></p>
<p><strong>读入后DataFrame的schema信息</strong></p>
<p><img src="/2020/05/23/SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/嵌套数据格式.png" style="zoom:50%;"></p>
<p>那么我们可以使用”列名.属性” 来访问相应的属性，即</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.sql(<span class="string">"select infos.age, infos.gender from students"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>jsonArray格式的数据</strong>，比如</p>
<p><img src="/2020/05/23/SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/json数组格式.png" alt></p>
<p><strong>读入后DataFrame的schema信息</strong></p>
<p><img src="/2020/05/23/SparkSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/json数组数据格式.png" style="zoom:50%;"></p>
<p>导入<strong>explode函数</strong>以及<strong>隐式转换</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._  <span class="comment">// 导入explode函数</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._                 <span class="comment">// 导入 “$” 和 as</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// explode函数将相应属性对应数组中的每个json数据分别和前面的数据组合，构成的新列名默认为 "col"</span></span><br><span class="line"><span class="keyword">val</span> transDF: <span class="type">DataFrame</span> = df.select($<span class="string">"name"</span>, $<span class="string">"age"</span>, explode($<span class="string">"scores"</span>)).toDF(<span class="string">"name"</span>, <span class="string">"age"</span>,<span class="string">"allScores"</span>)</span><br></pre></td></tr></table></figure></li>
<li><p><strong>创建视图</strong>的两种方式以及<strong>区别</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 导入spark对象下的所有方法，比如之后的spark.sql就可以直接写成sql</span></span><br><span class="line"><span class="keyword">import</span> spark._</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建当前会话下的视图</span></span><br><span class="line">df.createTempView(<span class="string">"students"</span>)</span><br><span class="line">df.createOrRepalceTempView(<span class="string">"studentsTest"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建全局视图</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"globalStudents"</span>)</span><br><span class="line">df.createOrRepalceGlobalTempView(<span class="string">"globalStudentsTest"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df1: <span class="type">DataFrame</span> = sql(<span class="string">"select * from students where age &gt; 18 and name = 'zhangsan5'"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 访问全局的表的时候需要加上global_temp.表名，这样就可以跨seeesion访问</span></span><br><span class="line"><span class="comment">// 用select语句中的where，可以用模糊匹配，比如列名 like 'wang%'</span></span><br><span class="line"><span class="comment">// 一个sc可以有多个会话，通过会话.newSession()来创建</span></span><br><span class="line">spark.newSession().sql(<span class="string">"select * from Students where name like 'wang%'"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个某个会话下的新视图会报错！！！</span></span><br><span class="line">spark.newSession().sql(<span class="string">"select * from students"</span>).show()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="读取json格式的RDD-DataSet"><a href="#读取json格式的RDD-DataSet" class="headerlink" title="读取json格式的RDD/DataSet"></a>读取json格式的RDD/DataSet</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建SparkSession对象</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().appName(<span class="string">"createDFFromJsonRDD"</span>).master(<span class="string">"local"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个的scala列表</span></span><br><span class="line"><span class="keyword">val</span> jsonList = <span class="type">List</span>[<span class="type">String</span>](</span><br><span class="line">  <span class="string">"&#123;\"name\":\"zhangsan\",\"age\":20&#125;"</span>,</span><br><span class="line">  <span class="string">"&#123;\"name\":\"lisi\",\"age\":21&#125;"</span>,</span><br><span class="line">  <span class="string">"&#123;\"name\":\"wangwu\",\"age\":22&#125;"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.6版本，创建json格式的RDD</span></span><br><span class="line"><span class="keyword">val</span> jsonRDD: <span class="type">RDD</span>[<span class="type">String</span>] = spark.sparkContext.makeRDD(jsonList)</span><br><span class="line"><span class="keyword">val</span> df1: <span class="type">DataFrame</span> = spark.read.json(jsonRDD)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.6+版本，创建json格式的DataSet</span></span><br><span class="line"><span class="comment">// 2.0+版本，只有读取json格式的DataSet</span></span><br><span class="line"><span class="keyword">val</span> jsonDs: <span class="type">Dataset</span>[<span class="type">String</span>] = jsonList.toDS()</span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">DataFrame</span> = spark.read.json(jsonDs)</span><br></pre></td></tr></table></figure>
<h3 id="读取RDD"><a href="#读取RDD" class="headerlink" title="读取RDD"></a>读取RDD</h3><ol>
<li><p>通过<strong>反射</strong>的方式将RDD转换成DataFrame</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建 SparkSession 对象</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="string">"ReflectionOrSchema"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建RDD</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = spark.sparkContext.textFile(<span class="string">"./data/people.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">id:<span class="type">Int</span>,name:<span class="type">String</span>,age:<span class="type">Int</span>,score:<span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">将RDD中的元素都变成样例类Person的对象</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">personRDD</span></span>: <span class="type">RDD</span>[<span class="type">Person</span>] = peopleRDD.map(one =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> arr: <span class="type">Array</span>[<span class="type">String</span>] = one.split(<span class="string">","</span>)</span><br><span class="line">  <span class="type">Person</span>(arr(<span class="number">0</span>).toInt, arr(<span class="number">1</span>).toString, arr(<span class="number">2</span>).toInt, arr(<span class="number">3</span>).toDouble)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为了使用rdd的toDF方法，必须要导入spark对象中的隐式转换对象</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将RDD通过.toDF()转化成DataFrame</span></span><br><span class="line"><span class="keyword">val</span> frame: <span class="type">DataFrame</span> = personRDD.toDF()</span><br></pre></td></tr></table></figure>
</li>
<li><p>动态创建<strong>Schema</strong></p>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建RDD</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = spark.sparkContext.textFile(<span class="string">"./data/people.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将peopleRDD转换成RDD[Row]</span></span><br><span class="line"><span class="keyword">val</span> rowRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = rdd.map(line =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> arr: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">","</span>)</span><br><span class="line">  <span class="type">Row</span>(arr(<span class="number">2</span>).toInt, arr(<span class="number">1</span>), arr(<span class="number">0</span>).toInt, arr(<span class="number">3</span>).toLong)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// row中的数据类型一定要与schema中的类型保持一致</span></span><br><span class="line"><span class="comment">// 可以动态地里面塞入列</span></span><br><span class="line"><span class="keyword">val</span> structType: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">List</span>[<span class="type">StructField</span>](</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>, nullable = <span class="literal">true</span>),</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, nullable = <span class="literal">true</span>),</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, nullable = <span class="literal">true</span>),</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"score"</span>, <span class="type">LongType</span>, nullable = <span class="literal">true</span>)</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="comment">// createDataFrame将二者自动映射</span></span><br><span class="line"><span class="comment">// 注：动态创建的ROW中数据的顺序要与Schema中的数据保持一致</span></span><br><span class="line"><span class="keyword">val</span> frame: <span class="type">DataFrame</span> = spark.createDataFrame(rowRDD, structType)</span><br></pre></td></tr></table></figure>
<h3 id="读取parquet格式数据加载DataFrame"><a href="#读取parquet格式数据加载DataFrame" class="headerlink" title="读取parquet格式数据加载DataFrame"></a>读取parquet格式数据加载DataFrame</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 和读取json文件的方式一样</span></span><br><span class="line"><span class="keyword">val</span> df1: <span class="type">DataFrame</span> = spark.read.parquet(<span class="string">"./data/parquet"</span>)</span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"parquet"</span>).load(<span class="string">"./data/parquet"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="读取csv格式数据加载DataFrame"><a href="#读取csv格式数据加载DataFrame" class="headerlink" title="读取csv格式数据加载DataFrame"></a>读取csv格式数据加载DataFrame</h3><ol>
<li>如果csv没有第一行，就只能指定schema了，即增加.schema(xxx)，并且列的顺序会和schema中列的顺序一致，其中xxx就是自己定义的schema</li>
<li>如果不指定schema都会解析成StringType</li>
<li>如果schema列数多于原始数据列数，那么多出来的会显示null，如果少于原始数据列数，那么只会取原始数据中前面的列，原始数据中多出的列会被忽略</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在读取csv文件的时候增加option选项来读取表头</span></span><br><span class="line"><span class="keyword">val</span> df1 : <span class="type">DataFrame</span> = spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"header"</span>, <span class="string">"true"</span>).load(<span class="string">"./data/test.csv"</span>)</span><br><span class="line"><span class="keyword">val</span> df2 : <span class="type">DataFrame</span> = spark.read.option(<span class="string">"header"</span>, <span class="string">"true"</span>).csv(<span class="string">"./data/test.csv"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="读取Mysql中的数据加载成DataFrame"><a href="#读取Mysql中的数据加载成DataFrame" class="headerlink" title="读取Mysql中的数据加载成DataFrame"></a>读取Mysql中的数据加载成DataFrame</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 方法1</span></span><br><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">properties.setProperty(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"password"</span>, <span class="string">"11111111"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//默认用mysql-connector-java里面的包</span></span><br><span class="line"><span class="keyword">val</span> person: <span class="type">DataFrame</span> = spark.read.jdbc(<span class="string">"jdbc:mysql://localhost:3306/spark"</span>, <span class="string">"person"</span>, properties)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法2</span></span><br><span class="line"><span class="comment">// 相比上一种方法可以显式指出来用了哪种包</span></span><br><span class="line"><span class="comment">// map中的键是指定写法，不能修改</span></span><br><span class="line"><span class="keyword">val</span> map: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">  <span class="string">"url"</span> -&gt; <span class="string">"jdbc:mysql://localhost:3306/spark"</span>,</span><br><span class="line">  <span class="string">"driver"</span>-&gt;<span class="string">"com.mysql.jdbc.Driver"</span>,</span><br><span class="line">  <span class="string">"user"</span>-&gt;<span class="string">"root"</span>,</span><br><span class="line">  <span class="string">"password"</span>-&gt;<span class="string">"11111111"</span>,</span><br><span class="line">  <span class="string">"dbtable"</span>-&gt;<span class="string">"score"</span> <span class="comment">//表名字</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> score: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"jdbc"</span>).options(map).load()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法3</span></span><br><span class="line"><span class="comment">// 相比于前一种方法，不用单独定义map</span></span><br><span class="line"><span class="keyword">val</span> reader: <span class="type">DataFrameReader</span> = spark.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://localhost:3306/spark"</span>)</span><br><span class="line">.option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"password"</span>, <span class="string">"11111111"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"score"</span>)</span><br><span class="line"><span class="keyword">val</span> score2: <span class="type">DataFrame</span> = reader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法4</span></span><br><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">properties.setProperty(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"password"</span>, <span class="string">"11111111"</span>)</span><br><span class="line"></span><br><span class="line">spark.read.jdbc(<span class="string">"jdbc:mysql://localhost:3306/spark"</span>, table = <span class="string">"(select person.id,person.name,person.age,score.score from person ,score where  person.id = score.id) T"</span>, properties)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将以上两张表注册临时表</span></span><br><span class="line">person.createOrReplaceTempView(<span class="string">"person"</span>)</span><br><span class="line">score.createOrReplaceTempView(<span class="string">"score"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过查询语句获得数据框</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = spark.sql(<span class="string">"select person.id,person.name,person.age,score.score from person ,score where  person.id = score.id"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 上述4种方法都支持将table转化成复杂语句，但是必须“（复杂语句）T” 给别名的方式来使用</span></span><br></pre></td></tr></table></figure>
<h3 id="读取Hive中的数据"><a href="#读取Hive中的数据" class="headerlink" title="读取Hive中的数据"></a>读取Hive中的数据</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Spark1.6 中用HiveContext对象操作Hive数据</span></span><br><span class="line"><span class="comment">// Spark2.0+ 中用SparkSession对象操作Hive数据，SparkSession将SQLContext和HiveContext封装起来，但是读取Hive中的数据要开启Hive支持，即enableHiveSupport()</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">"CreateDataFrameFromHive"</span>)</span><br><span class="line">.master(<span class="string">"local"</span>)   <span class="comment">// 读取本地Hive仓库中的数据</span></span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"show databases"</span>)</span><br><span class="line">spark.sql(<span class="string">"use spark"</span>)</span><br><span class="line">spark.sql(<span class="string">"show tables"</span>)</span><br><span class="line">spark.sql(<span class="string">"drop table if exists student_infos"</span>)</span><br><span class="line">spark.sql(<span class="string">"create table if not exists student_infos(name string, age int) row format delimited fields terminated by '\t'"</span>)</span><br><span class="line">spark.sql(<span class="string">"load data local inpath './data/student_infos' into table student_infos"</span>)</span><br><span class="line">spark.sql(<span class="string">"drop table if exists student_scores"</span>)</span><br><span class="line">spark.sql(<span class="string">"create table if not exists student_scores ( name string, score int) row format delimited fields terminated by '\t'"</span>)</span><br><span class="line">spark.sql(<span class="string">"load data local inpath './data/student_scores' into table student_scores"</span>)</span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = spark.sql(<span class="string">"select si.name, si.age, ss.score from student_infos si, student_scores ss where si.name = ss.name"</span>)</span><br><span class="line">spark.sql(<span class="string">"drop table if exists good_student_infos"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据框保存带hive表中</span></span><br><span class="line">result.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"good_student_infos"</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个具有多种数据格式的hive表</span></span><br><span class="line">spark.sql(<span class="string">"drop table if exists t"</span>)</span><br><span class="line">spark.sql(<span class="string">"create table if not exists t(id struct&lt;id1:int,id2:int,id3:int&gt;,name array&lt;string&gt;,xx map&lt;int,string&gt;) "</span> +</span><br><span class="line">          <span class="string">" row format delimited "</span> +</span><br><span class="line">          <span class="string">"fields terminated by ' ' "</span> +</span><br><span class="line">          <span class="string">"collection items terminated by ',' "</span> +</span><br><span class="line">          <span class="string">"map keys terminated by ':' "</span> +</span><br><span class="line">          <span class="string">"lines terminated by '\n'"</span>)</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"load data local inpath './data/student_t' overwrite into table t"</span>)</span><br><span class="line"><span class="keyword">val</span> result2: <span class="type">DataFrame</span> = spark.sql(<span class="string">"select * from t"</span>)</span><br><span class="line">result2.show(truncate = <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<p><strong>补充</strong>：</p>
<ol>
<li><p>Hive是建立在<strong>HDFS</strong>上的数据仓库架构，并且对存储在<strong>HDFS</strong>中的数据进行分析和管理</p>
</li>
<li><p>Hive建表语法中分隔符：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建一张hive表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t(<span class="keyword">id</span> <span class="keyword">struct</span>&lt;id1:<span class="built_in">int</span>,id2:<span class="built_in">int</span>,id3:<span class="built_in">int</span>&gt;,<span class="keyword">name</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,xx <span class="keyword">map</span>&lt;<span class="built_in">int</span>,<span class="keyword">string</span>&gt;)     							</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>               <span class="comment">-- 分隔符设置开始语句，必须放最前</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>          <span class="comment">-- 设置字段与字段之间的分隔符</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> <span class="comment">-- 设置复杂类型（array,struct)字段中各个item之间的分隔符                 </span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span>         <span class="comment">-- 设置复杂类型(Map)字段中key和value之间的分隔符</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span>;          <span class="comment">-- 设置行与行之间的分隔符，必须放最后</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Hive的<strong>load语法</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- load基本语法</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> [<span class="keyword">local</span>] inpath <span class="string">'filepath'</span> [overwrite]</span><br><span class="line"><span class="keyword">into</span> <span class="keyword">table</span> tablename [<span class="keyword">partition</span> (partcol1=val1,partcol2=val2...)]</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 方式1. 将本地的数据文件导入上步创建的t表，并且覆盖原来的数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'./data/student_t.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 方式2. 将HDFS中的数据文件导入上步创建的t表，并且覆盖原来的数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/i/student_t.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> t3</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="DataFrame原生API"><a href="#DataFrame原生API" class="headerlink" title="DataFrame原生API"></a>DataFrame原生API</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"./data/json"</span>) <span class="comment">// 将json文件加载成DataFrame</span></span><br><span class="line">df.printSchema()  									<span class="comment">// 打印schema信息</span></span><br><span class="line"><span class="keyword">val</span> row1: <span class="type">Row</span> = df.first() 					<span class="comment">// 获取第1行数据</span></span><br><span class="line"><span class="keyword">val</span> rows2: <span class="type">Array</span>[<span class="type">Row</span>] = df.take(<span class="number">4</span>) 	<span class="comment">// 获取前4数据</span></span><br><span class="line"><span class="keyword">val</span> rows3: <span class="type">Array</span>[<span class="type">Row</span>] = df.head(<span class="number">4</span>) 	<span class="comment">// 获取前4数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 展示前rownum行的数据(默认展示前20行)，truncate=false 表示将数据展开</span></span><br><span class="line">df.show(rownum, truncate=<span class="literal">false</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询方法</span></span><br><span class="line"><span class="keyword">val</span> df1: <span class="type">DataFrame</span> = df.select(<span class="string">"name"</span>) </span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">DataFrame</span> = df.select(<span class="string">"name"</span>, <span class="string">"age"</span>) </span><br><span class="line"><span class="keyword">val</span> df3: <span class="type">DataFrame</span> = df.select($<span class="string">"name"</span>.as(<span class="string">"studentNanme"</span>), $<span class="string">"age"</span>)</span><br><span class="line"><span class="keyword">val</span> df4: <span class="type">DataFrame</span> = df.select($<span class="string">"name"</span>, ($<span class="string">"age"</span> + <span class="number">1</span>).as(<span class="string">"addAge"</span>))</span><br><span class="line"><span class="keyword">val</span> df5: <span class="type">DataFrame</span> = df.select(df.col(<span class="string">"name"</span>) , (df.col(<span class="string">"age"</span>) + <span class="number">1</span>).as(<span class="string">"addAge"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 过滤方法</span></span><br><span class="line"><span class="keyword">val</span> df1: <span class="type">Dataset</span>[<span class="type">Row</span>] = df.filter(<span class="string">"age &gt; 18"</span>) </span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">Dataset</span>[<span class="type">Row</span>] = df.filter($<span class="string">"age"</span>&gt;<span class="number">18</span>)</span><br><span class="line"><span class="keyword">val</span> df3: <span class="type">Dataset</span>[<span class="type">Row</span>] = df.filter(df.col(<span class="string">"age"</span>)&gt;<span class="number">18</span>)</span><br><span class="line"><span class="keyword">val</span> df4: <span class="type">Dataset</span>[<span class="type">Row</span>] = df.filter(df.col(<span class="string">"name"</span>).equalTo(<span class="string">"zhangsan"</span>))</span><br><span class="line"><span class="keyword">val</span> df5: <span class="type">Dataset</span>[<span class="type">Row</span>] = df.filter(<span class="string">"name = 'zhangsan4' or name = 'zhangsan5'"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 排序方法</span></span><br><span class="line"><span class="keyword">val</span> df1: <span class="type">Dataset</span>[<span class="type">Row</span>] = df.sort($<span class="string">"age"</span>.asc, $<span class="string">"name"</span>.desc)</span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">Dataset</span>[<span class="type">Row</span>] = df.sort(df.col(<span class="string">"age"</span>).asc, df.col(<span class="string">"name"</span>).desc)</span><br><span class="line"><span class="keyword">val</span> df3: <span class="type">DataFrame</span> = df.select(df.col(<span class="string">"name"</span>).as(<span class="string">"studentName"</span>), df.col(<span class="string">"age"</span>).alias(<span class="string">"studentAge"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分组方法</span></span><br><span class="line"><span class="keyword">val</span> df1: <span class="type">DataFrame</span> = df.groupBy(<span class="string">"age"</span>).count() </span><br><span class="line"></span><br><span class="line"><span class="comment">// map方法</span></span><br><span class="line"><span class="comment">// 数据框中的每一行都是Row类型，它可以像数组一样可以通过下标获取值</span></span><br><span class="line"><span class="comment">// Row类型相比数组的优点是保存列信息，也就是可以根据row中的字段名称获取值</span></span><br><span class="line"><span class="keyword">val</span> df1: <span class="type">DataFrame</span> = df.map(row =&gt; <span class="string">"Name: "</span> + row(<span class="number">0</span>)).show()</span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">DataFrame</span> = df.map(row =&gt; <span class="string">"name: "</span> + row.getAs[<span class="type">String</span>](<span class="string">"name"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// join方法: 每一种连接方法都会将两个连接键保留</span></span><br><span class="line"><span class="comment">// 1. 内连接</span></span><br><span class="line"><span class="keyword">val</span> inner_df1 = df1.join(df2, df1(<span class="string">"name"</span>) === df2(<span class="string">"name"</span>))</span><br><span class="line"><span class="keyword">val</span> inner_df2 = df1.join(df2, df1(<span class="string">"name"</span>) === df2(<span class="string">"name"</span>), joinType = <span class="string">"inner"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 左连接</span></span><br><span class="line"><span class="keyword">val</span> left_df1 = df1.join(df2, df1(<span class="string">"name"</span>) === df2(<span class="string">"name"</span>), joinType = <span class="string">"left"</span>)</span><br><span class="line"><span class="keyword">val</span> left_df2 = df1.join(df2, df1(<span class="string">"name"</span>) === df2(<span class="string">"name"</span>), joinType = <span class="string">"left_outer"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 右连接</span></span><br><span class="line"><span class="keyword">val</span> right_df1 = df1.join(df2, df1(<span class="string">"name"</span>) === df2(<span class="string">"name"</span>), joinType = <span class="string">"right"</span>)</span><br><span class="line"><span class="keyword">val</span> right_df2 = df1.join(df2, df1(<span class="string">"name"</span>) === df2(<span class="string">"name"</span>), joinType = <span class="string">"right_outer"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. 全连接</span></span><br><span class="line"><span class="keyword">val</span> outer_df1 = df1.join(df2, df1(<span class="string">"name"</span>) === df2(<span class="string">"name"</span>), joinType = <span class="string">"full"</span>)</span><br><span class="line"><span class="keyword">val</span> outer_df2 = df1.join(df2, df1(<span class="string">"name"</span>) === df2(<span class="string">"name"</span>), joinType = <span class="string">"outer"</span>)</span><br><span class="line"><span class="keyword">val</span> outer_df3 = df1.join(df2, df1(<span class="string">"name"</span>) === df2(<span class="string">"name"</span>), joinType = <span class="string">"full_outer"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据框列名的重命名</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._ </span><br><span class="line"><span class="keyword">val</span> temp: <span class="type">DataFrame</span> = df.withColumnRenamed(<span class="string">"name"</span>, <span class="string">"people_name"</span>)</span><br><span class="line"><span class="keyword">val</span> temp: <span class="type">DataFrame</span> = idMappingDF.select($<span class="string">"name"</span>.alias(<span class="string">"people_name"</span>))</span><br><span class="line"><span class="keyword">val</span> temp: <span class="type">DataFrame</span> = idMappingDF.select($<span class="string">"name"</span>.as(<span class="string">"people_name"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrame转成Row类型的RDD</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Row</span>] = df.rdd</span><br><span class="line">rdd.foreach(row =&gt; &#123;</span><br><span class="line">  <span class="comment">//打印row</span></span><br><span class="line">  println(row)</span><br><span class="line">  <span class="comment">// row中获取值的两种方式：1.getAs("字段名") 2.getAs(下标)</span></span><br><span class="line">  <span class="keyword">val</span> nameField: <span class="type">String</span> = row.getAs[<span class="type">String</span>](<span class="string">"name"</span>)</span><br><span class="line">  <span class="keyword">val</span> ageField: <span class="type">Long</span> = row.getAs[<span class="type">Long</span>](<span class="string">"age"</span>)</span><br><span class="line">  <span class="keyword">val</span> nameIndex: <span class="type">String</span> = row.getAs[<span class="type">String</span>](<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">val</span> ageIndex: <span class="type">Long</span> = row.getAs[<span class="type">Long</span>](<span class="number">0</span>)</span><br><span class="line">  println(<span class="string">s"name = <span class="subst">$nameIndex</span>, age = <span class="subst">$ageIndex</span>"</span>)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>总结：</p>
<ul>
<li>filter/sort 得到的结果是DataSet</li>
<li>df.col(“xxx”) 和 $xxx的作用相同</li>
<li>Dataset和DataFrame都有show方法</li>
<li>select中如果其中一列用到了隐式转换$，其他列也必须用隐式转换</li>
</ul>
<h2 id="保存DataFrame"><a href="#保存DataFrame" class="headerlink" title="保存DataFrame"></a>保存DataFrame</h2><h3 id="将DataFrame保存成parquet文件"><a href="#将DataFrame保存成parquet文件" class="headerlink" title="将DataFrame保存成parquet文件"></a>将DataFrame保存成parquet文件</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).format(<span class="string">"parquet"</span>).save(<span class="string">"./data/parquet"</span>)</span><br><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).parquet(<span class="string">"./data/parquet"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="将DataFrame保存成csv文件"><a href="#将DataFrame保存成csv文件" class="headerlink" title="将DataFrame保存成csv文件"></a>将DataFrame保存成csv文件</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).format(<span class="string">"csv"</span>).save(<span class="string">"./data/test.csv"</span>)</span><br><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).csv(<span class="string">"./data/test.csv"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="将DataFrame保存到MySQL表中"><a href="#将DataFrame保存到MySQL表中" class="headerlink" title="将DataFrame保存到MySQL表中"></a>将DataFrame保存到MySQL表中</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 注：数据框的名称result在jdbc()中要写成字符串的形式！！！</span></span><br><span class="line">result.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).jdbc(<span class="string">"jdbc:mysql://localhost:3306/spark"</span>, <span class="string">"result"</span>, properties)</span><br></pre></td></tr></table></figure>
<h3 id="将DataFrame保存到Hive表中"><a href="#将DataFrame保存到Hive表中" class="headerlink" title="将DataFrame保存到Hive表中"></a>将DataFrame保存到Hive表中</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 保存到本地的hive表中</span></span><br><span class="line">result2.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"t_result"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="UDF-和-UDAF"><a href="#UDF-和-UDAF" class="headerlink" title="UDF 和 UDAF"></a>UDF 和 UDAF</h1><h2 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h2><p>user defined function(用户自定义函数)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建spark对象</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .master(<span class="string">"local"</span>)</span><br><span class="line">  .appName(<span class="string">"UDF"</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line">  <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> nameList: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>[<span class="type">String</span>](<span class="string">"zhangsan"</span>, <span class="string">"lisi"</span>, <span class="string">"wangwu"</span>, <span class="string">"zhaoliu"</span>, <span class="string">"tianqi"</span>)</span><br><span class="line"><span class="keyword">val</span> nameDF: <span class="type">DataFrame</span> = nameList.toDF(<span class="string">"name"</span>)</span><br><span class="line">nameDF.createOrReplaceTempView(<span class="string">"students"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过udf自定义计算字符串长度的函数</span></span><br><span class="line">spark.udf.register(<span class="string">"STRLEN"</span>, (n:<span class="type">String</span>)=&gt;&#123;</span><br><span class="line">  n.length</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// hive中的order by是做全局排序（也就一个reducer），而sort by是分区排序，可以指定reducer的数量</span></span><br><span class="line">spark.sql(<span class="string">"select name, STRLEN(name) as length from students sort by length desc"</span>).show(<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<h2 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h2><p>user defined aggregate function，用户自定义聚合函数，常用的聚合函数有count,avg,sum,min,max.. 特点：多对一</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 聚合函数必须和group by一起使用</span></span><br><span class="line"><span class="comment">-- 和聚合函数一起出现的字段，必须要出新在group by的后面</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span> ,<span class="keyword">count</span>(*) <span class="keyword">from</span> <span class="keyword">table</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">是聚合函数，要继承 <span class="type">UserDefinedAggregateFunction</span>() 实现<span class="number">8</span>个方法，最重要三个方法</span><br><span class="line">    initialize</span><br><span class="line">    update</span><br><span class="line">    merge</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tagstag/Spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BD%9C%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag"># Spark学习记录｜计算机学习笔记</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/23/SparkCore%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="prev" title="SparkCore学习笔记">
      <i class="fa fa-chevron-left"></i> SparkCore学习笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/06/17/Scala%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="next" title="Scala学习笔记">
      Scala学习笔记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81MDU4NS8yNzA2OA=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkSQL的发展"><span class="nav-number">1.</span> <span class="nav-text">SparkSQL的发展</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#与Hive的区别"><span class="nav-number">1.1.</span> <span class="nav-text">与Hive的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#与Shark的区别"><span class="nav-number">1.2.</span> <span class="nav-text">与Shark的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#引入了谓词下推来优化job"><span class="nav-number">1.3.</span> <span class="nav-text">引入了谓词下推来优化job</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#版本1-6和2-0-的区别"><span class="nav-number">1.4.</span> <span class="nav-text">版本1.6和2.0+的区别</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DataSet"><span class="nav-number">2.</span> <span class="nav-text">DataSet</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#与RDD的区别"><span class="nav-number">2.1.</span> <span class="nav-text">与RDD的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建DataSet"><span class="nav-number">2.2.</span> <span class="nav-text">创建DataSet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实现WordCount"><span class="nav-number">2.3.</span> <span class="nav-text">实现WordCount</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DataFrame"><span class="nav-number">3.</span> <span class="nav-text">DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#创建DataFrame"><span class="nav-number">3.1.</span> <span class="nav-text">创建DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#读取json格式的文件"><span class="nav-number">3.1.1.</span> <span class="nav-text">读取json格式的文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读取json格式的RDD-DataSet"><span class="nav-number">3.1.2.</span> <span class="nav-text">读取json格式的RDD&#x2F;DataSet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读取RDD"><span class="nav-number">3.1.3.</span> <span class="nav-text">读取RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读取parquet格式数据加载DataFrame"><span class="nav-number">3.1.4.</span> <span class="nav-text">读取parquet格式数据加载DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读取csv格式数据加载DataFrame"><span class="nav-number">3.1.5.</span> <span class="nav-text">读取csv格式数据加载DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读取Mysql中的数据加载成DataFrame"><span class="nav-number">3.1.6.</span> <span class="nav-text">读取Mysql中的数据加载成DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读取Hive中的数据"><span class="nav-number">3.1.7.</span> <span class="nav-text">读取Hive中的数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrame原生API"><span class="nav-number">3.2.</span> <span class="nav-text">DataFrame原生API</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#保存DataFrame"><span class="nav-number">3.3.</span> <span class="nav-text">保存DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#将DataFrame保存成parquet文件"><span class="nav-number">3.3.1.</span> <span class="nav-text">将DataFrame保存成parquet文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#将DataFrame保存成csv文件"><span class="nav-number">3.3.2.</span> <span class="nav-text">将DataFrame保存成csv文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#将DataFrame保存到MySQL表中"><span class="nav-number">3.3.3.</span> <span class="nav-text">将DataFrame保存到MySQL表中</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#将DataFrame保存到Hive表中"><span class="nav-number">3.3.4.</span> <span class="nav-text">将DataFrame保存到Hive表中</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#UDF-和-UDAF"><span class="nav-number">4.</span> <span class="nav-text">UDF 和 UDAF</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#UDF"><span class="nav-number">4.1.</span> <span class="nav-text">UDF</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#UDAF"><span class="nav-number">4.2.</span> <span class="nav-text">UDAF</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="黄宁"
      src="/images/%E4%BA%91%E5%8D%97%E5%B1%B1%E9%97%B4%E8%87%AA%E6%8B%8D.jpg">
  <p class="site-author-name" itemprop="name">黄宁</p>
  <div class="site-description" itemprop="description">普通人也可以活得很精彩</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/HuangNing616" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HuangNing616" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:nnkawayi@qq.com" title="E-Mail → mailto:nnkawayi@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/demons-28-75-48" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;demons-28-75-48" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>知乎</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.nowcoder.com/ta/coding-interviews" title="牛客网 → https:&#x2F;&#x2F;www.nowcoder.com&#x2F;ta&#x2F;coding-interviews" rel="noopener" target="_blank"><i class="fa fa-fw fa-google"></i>牛客网</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">黄宁</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.1.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.6.0
  </div>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.7}});</script></body>
</html>
